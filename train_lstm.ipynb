{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train our RNN on extracted features or images.\n",
    "\"\"\"\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "# from models import ResearchModels\n",
    "from data import DataSet\n",
    "import time\n",
    "import os.path\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Bidirectional\n",
    "import sys\n",
    "from keras.layers import Dense, Flatten, Dropout, ZeroPadding3D\n",
    "\n",
    "def train(data_type, seq_length, model, saved_model=None,\n",
    "          class_limit=None, image_shape=None,\n",
    "          load_to_memory=False, batch_size=32, nb_epoch=100):\n",
    "    # Helper: Save the model.\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        filepath=os.path.join('data', 'checkpoints', model + '-' + data_type + \\\n",
    "            '.{epoch:03d}-{val_loss:.3f}.hdf5'),\n",
    "        verbose=1,\n",
    "        save_best_only=True)\n",
    "\n",
    "    # Helper: TensorBoard\n",
    "    tb = TensorBoard(log_dir=os.path.join('data', 'logs', model))\n",
    "\n",
    "    # Helper: Stop when we stop learning.\n",
    "    early_stopper = EarlyStopping(patience=5)\n",
    "\n",
    "    # Helper: Save results.\n",
    "    timestamp = time.time()\n",
    "    csv_logger = CSVLogger(os.path.join('data', 'logs', model + '-' + 'training-' + \\\n",
    "        str(timestamp) + '.log'))\n",
    "\n",
    "    # Get the data and process it.\n",
    "    if image_shape is None:\n",
    "        data = DataSet(\n",
    "            seq_length=seq_length,\n",
    "            class_limit=class_limit\n",
    "        )\n",
    "    else:\n",
    "        data = DataSet(\n",
    "            seq_length=seq_length,\n",
    "            class_limit=class_limit,\n",
    "            image_shape=image_shape\n",
    "        )\n",
    "\n",
    "    # Get samples per epoch.\n",
    "    # Multiply by 0.7 to attempt to guess how much of data.data is the train set.\n",
    "    steps_per_epoch = (len(data.data) * 0.7) // batch_size\n",
    "\n",
    "    if load_to_memory:\n",
    "        # Get data.\n",
    "        X, y = data.get_all_sequences_in_memory('train', data_type)\n",
    "        X_test, y_test = data.get_all_sequences_in_memory('test', data_type)\n",
    "    else:\n",
    "        # Get generators.\n",
    "        generator = data.frame_generator(batch_size, 'train', data_type)\n",
    "        val_generator = data.frame_generator(batch_size, 'test', data_type)\n",
    "\n",
    "    # Get the model.\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Bidrectional(LSTM(2048, return_sequences=False,\n",
    "                       input_shape=(seq_length, 2048),\n",
    "                       dropout=0.5)))\n",
    "    lstm_model.add(Dense(512, activation='relu'))\n",
    "    lstm_model.add(Dropout(0.5))\n",
    "    lstm_model.add(Dense(len(data.classes), activation='softmax'))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(n_timesteps, 1)))\n",
    "    model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    optimizer = Adam(lr=1e-5, decay=1e-6)\n",
    "    metrics = ['accuracy']\n",
    "    metrics.append('top_k_categorical_accuracy')\n",
    "    lstm_model.model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=metrics)\n",
    "    \n",
    "    print(lstm_model.model.summary())\n",
    "\n",
    "    # Fit!\n",
    "    if load_to_memory:\n",
    "        # Use standard fit.\n",
    "        lstm_model.model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            verbose=1,\n",
    "            callbacks=[tb, early_stopper, csv_logger],\n",
    "            epochs=nb_epoch)\n",
    "    else:\n",
    "        # Use fit generator.\n",
    "        lstm_model.model.fit_generator(\n",
    "            generator=generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=nb_epoch,\n",
    "            verbose=1,\n",
    "            callbacks=[tb, early_stopper, csv_logger, checkpointer],\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=40,\n",
    "            workers=4)\n",
    "        \n",
    "\n",
    "def main():\n",
    "    \"\"\"These are the main training settings. Set each before running\n",
    "    this file.\"\"\"\n",
    "   \n",
    "    saved_model = None  # None or weights file\n",
    "    class_limit = None  # int, can be 1-101 or None\n",
    "    seq_length = 40\n",
    "    load_to_memory = False  # pre-load the sequences into memory\n",
    "    batch_size = 32\n",
    "    nb_epoch = 1000\n",
    "    data_type = 'features'\n",
    "    image_shape = None\n",
    "    \n",
    "    \n",
    "    train(data_type, seq_length, model, saved_model=saved_model,\n",
    "          class_limit=class_limit, image_shape=image_shape,\n",
    "          load_to_memory=load_to_memory, batch_size=batch_size, nb_epoch=nb_epoch)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
